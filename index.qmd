\
---
title: "Итоговый проект — Вариант 1: корпусный анализ с визуализацией"
date: "19.12.2025"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
---

## Что это за работа

Я собрала небольшой корпус текстов на европейском языке (английский): 20 одностраничных PDF-документов.  
Дальше я сделала пайплайн: OCR → очистка → лемматизация и частоты → коллокации и визуализация → отчёт в Quarto.

---

## Данные и структура проекта

- Документов: **20**
- Язык: **английский**
- Формат исходников: **PDF (1 страница на документ)**

Папки проекта:

- `data_raw/` — исходные PDF  
- `data_ocr/` — результат OCR (`.txt`)  
- `data_clean/` — очищенные тексты (`.txt`)  
- `outputs/tables/` — таблицы со статистикой  
- `outputs/figures/` — картинки (графики/сеть)  
- `R/` — скрипты пайплайна

---

## Шаг 1. OCR (tesseract)

Для каждого PDF я делала так:

1) Конвертировала страницу PDF в PNG (300 dpi).  
2) Распознавала текст через `tesseract`.

На выходе получилось 20 файлов в `data_ocr/`.

```{r}
length(list.files("data_ocr", pattern = "\\.txt$", full.names = TRUE))
```

---

## Шаг 2. Очистка текста регулярными выражениями

После OCR в тексте обычно есть мусор: странные переносы, лишние пробелы, артефакты форматирования.
Я почистила это регулярными выражениями, чтобы потом нормально считать статистику.

Что делала (основные правила):

* нормализация переносов строк (`\\r\\n` → `\\n`)
* склейка переносов с дефисом: `exam-\\nple → example`
* схлопывание лишних пробелов
* приведение пустых строк к аккуратным абзацам

### Пример до/после (кусок одного файла)

```{r}
f_raw <- list.files("data_ocr", pattern="\\.txt$", full.names=TRUE)[1]
f_clean <- file.path("data_clean", basename(f_raw))

cat("=== ДО ОЧИСТКИ (OCR) ===\n")
cat(paste(readLines(f_raw, warn=FALSE)[1:10], collapse="\n"))

cat("\n\n=== ПОСЛЕ ОЧИСТКИ (clean) ===\n")
cat(paste(readLines(f_clean, warn=FALSE)[1:10], collapse="\n"))
```

---

## Шаг 3. UDPipe: лемматизация и частотный словарь

Очищенные тексты я разметила через `udpipe` (модель: `english-ewt`).
Потом я сделала частотный словарь по **леммам**.

Леммы мне нужны, потому что одно и то же слово может быть в разных формах, и так проще считать.

### Топ-50 лемм

```{r}
library(readr)

lemma_path <- "outputs/tables/lemma_freq_top50.csv"
if (!file.exists(lemma_path)) {
  stop("Не найден файл: ", lemma_path, "\nПроверь, что пайплайн создал outputs/tables/lemma_freq_top50.csv")
}
lemma_top50 <- read_csv(lemma_path, show_col_types = FALSE)
lemma_top50
```

Файлы шага:

* `outputs/tables/udpipe_tokens.csv`
* `outputs/tables/lemma_freq_total.csv`
* `outputs/tables/lemma_freq_top50.csv`

---

## Шаг 4. Коллокации и визуализация

Дальше я искала коллокации (устойчивые сочетания).
Я брала **биграммы** (соседние леммы внутри предложения) и считала PMI.

PMI показывает, насколько пара слов встречается вместе чаще, чем “случайно”.

### Топ-20 коллокаций по PMI

![](outputs/figures/collocations_bar.png)

В топе получились нормальные пары (типа **“food poisoning”**, **“cable tv”**, **“old people”**), а ещё частые связки вроде **“of course”**, **“do not”**, **“such as”**.

### Сеть коллокаций (top-50)

![](outputs/figures/collocations_network.png)

Сеть просто помогает посмотреть, какие слова часто “цепляются” друг за друга и образуют группы.

Полная таблица коллокаций:

```{r}
library(dplyr)
library(readr)

coll_path <- "outputs/tables/collocations_pmi.csv"
if (!file.exists(coll_path)) {
  stop("Не найден файл: ", coll_path, "\nПроверь, что пайплайн создал outputs/tables/collocations_pmi.csv")
}
coll <- read_csv(coll_path, show_col_types = FALSE)
coll %>% head(10)
```

---

## Итог

1. Я сделала OCR для 20 PDF и получила тексты.
2. Я почистила тексты регулярками.
3. Я построила частотный словарь лемм.
4. Я нашла коллокации и сделала две визуализации: топ-20 по PMI и сеть.

---
